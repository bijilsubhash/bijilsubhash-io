<!DOCTYPE html>
<html lang="en-us">

<head><script src="/livereload.js?mindelay=10&amp;v=2&amp;port=1313&amp;path=livereload" data-no-instant defer></script>
    <title>
Lessons from Building Data Pipelines on DLT: Part 1 | Bijil Subhash
</title>

    <meta http-equiv="content-type" content="text/html; charset=utf-8">
<meta name="viewport" content="width=device-width, initial-scale=1.0">
<meta name="description" content="">

<meta name="generator" content="Hugo 0.123.3">


<link rel="canonical" href="http://localhost:1313/blog/lessons-from-building-data-pipelines-on-dlt/" >




<link href="/css/style.min.d23c1980faa61c0d6b331a0cd9dbc34fb7dade9e294067f1108fb8f26bd7796c.css" rel="stylesheet">




</head>

<body>

    <div class="flexWrapper">
        <script src="https://cdn.jsdelivr.net/npm/mermaid/dist/mermaid.min.js"></script>
<script>
    let isDark = window.matchMedia('(prefers-color-scheme: dark)').matches;
    let mermaidTheme = 'dark';
    let mermaidConfig = {
        theme: mermaidTheme,
        logLevel: 'fatal',
        securityLevel: 'strict',
        startOnLoad: true,
        arrowMarkerAbsolute: false,

        er: {
            diagramPadding: 20,
            layoutDirection: 'TB',
            minEntityWidth: 100,
            minEntityHeight: 75,
            entityPadding: 15,
            stroke: 'gray',
            fill: 'honeydew',
            fontSize: 12,
            useMaxWidth: true,
        },
        flowchart: {
            diagramPadding: 8,
            htmlLabels: true,
            curve: 'basis',
            padding: 50,
            align: 'center',
            fontFamily: '"Open-Sans", "sans-serif"',

        },
        sequence: {
            diagramMarginX: 50,
            diagramMarginY: 10,
            actorMargin: 50,
            width: 150,
            height: 65,
            boxMargin: 10,
            boxTextMargin: 5,
            noteMargin: 10,
            messageMargin: 35,
            messageAlign: 'center',
            mirrorActors: true,
            bottomMarginAdj: 1,
            useMaxWidth: true,
            rightAngles: false,
            showSequenceNumbers: false,
        },
        gantt: {
            titleTopMargin: 25,
            barHeight: 20,
            barGap: 4,
            topPadding: 50,
            leftPadding: 75,
            gridLineStartPadding: 35,
            fontSize: 11,
            fontFamily: '"Open-Sans", "sans-serif"',
            numberSectionStyles: 4,
            axisFormat: '%Y-%m-%d',
            topAxis: false,
        },
    };
    mermaid.initialize(mermaidConfig);
</script>
<style>
    .postDate {
        float: right;
        color: #f8f8ff;
    }
    .listHeader {
        text-align: left;
        margin-left: 20px;
    }

    @media screen and (max-width: 768px) {
        .postDate {
            float: left;
            width: 100%;
            text-align: left;
        }

        .postTitle {
            float: left;
            width: 100%;
            text-align: left;
        }

        .listHeader {
            float: none;
            width: 100%;
            text-align: center;
            margin-left: 0;
        }
    }
</style>

<header class="headerWrapper">
    <div class="header">
        <div>
            <a class="terminal" href="http://localhost:1313/">
                <span>hello@bijilsubhash.io ~ $</span>
            </a>
        </div>
        <input class="side-menu" type="checkbox" id="side-menu">
        <label class="hamb" for="side-menu"><span class="hamb-line"></span></label>
        <nav class="headerLinks">
            <ul>
                
                <li>
                    <a href="http://localhost:1313/about" title="" >
                        ~/about</a>
                </li>
                
                <li>
                    <a href="http://localhost:1313/blog" title="" >
                        ~/blog</a>
                </li>
                
            </ul>
        </nav>
    </div>
</header>


        <div class="content">
            <main class="main">
                
<div class="postWrapper">
    <h1>Lessons from Building Data Pipelines on DLT: Part 1</h1>
    
    
    <section class="postMetadata">
        <dl>
            
                
<dt>tags</dt>
<dd><span></span>
    <a href="/tags/data-ingestion/">#Data Ingestion</a><span></span>
    <a href="/tags/dlt/">#DLT</a></dd>
            
            
            
            
                <dt>published</dt>
                
                <dd><time datetime="2024-11-03">November 3, 2024</time></dd>
            
            
                <dt>reading time</dt>
                <dd>4 minutes</dd>
            
        </dl>
    </section>
    
    <div>
        <p>Couple of months ago, I <a href="https://bijilsubhash.io/blog/exploring-dlt-for-data-ingestion/">shared</a> my initial experience with Data Load Tool (DLT). Since then, I have been building some projects with the help of their <a href="https://dlthub.com/docs/intro">documentation</a> and the active Slack community. The Python-first approach of DLT greatly appealed to me as a data engineer. Over the past two months, I’ve deepened my appreciation for idiomatic Python while exploring and applying several of DLT’s features, which I am m eager to leverage in real-world projects. In this article, I’ll dive into some key DLT features and the lessons learned from building data pipelines.</p>
<p>Note: While this article predominantly refers to the use of RESTful APIs as data sources, DLT&rsquo;s versatility extends to many other options, which I will discuss in future posts.</p>
<h1 id="restclient">RESTClient</h1>
<p>DLT offers <a href="https://dlthub.com/docs/devel/dlt-ecosystem/verified-sources/rest_api/basic">REST API source</a>, which is a declarative framework to define API endpoints that has dedicated configuration parameter for handling pagination and authentication. REST API source is powered by <a href="https://dlthub.com/docs/devel/general-usage/http/rest-client">RESTClient</a> class, which has a paginate method for fetching data from API responses. Generally, if you were building an ingestion pipeline using Python, you are likely writing a custom method for handling pagination and authentication. Luckily, DLT has a few immensely flexible classes that allows you to handle these scenarios elegantly. At the same time, you also have the option to build custom paginators and authenticators if there is a need for it. Additionally, you have the option to pass <a href="https://github.com/h2non/jsonpath-ng?tab=readme-ov-file#jsonpath-syntax">JSONPath</a> selectors as a parameter to specify what data should be extracted from deeply nested responses.  Overall, working with RESTClient gives you the opportunity to build reliable pipelines seamlessly.</p>
<h1 id="incremental-load">Incremental Load</h1>
<p>We live in a world where we have cheap cloud storage but the compute cost remains high. It is, therefore, in our best interest to optimize the data ingestion pipelines such that the amount of compute required to process the data is minimized. To this end, a strategy that is commonly implemented is to load the data incrementally i.e. only ingest new data as opposed to full load when a pipeline is triggered. Fortunately, DLT supports <a href="https://dlthub.com/docs/general-usage/incremental-loading#incremental-loading-with-a-cursor-field">incremental load with a cursor field</a>, which in a nutshell uses a field (usually a date field) from the response to track the state and is made available on the next pipeline run to extract only the most recent values. I highly recommend playing around with this feature to get a thorough understanding of all the nuances associated with setting up incremental load in DLT.</p>
<h1 id="destination">Destination</h1>
<p>We have a few options when it comes to <a href="https://dlthub.com/docs/dlt-ecosystem/destinations/">destinations</a> in DLT, such as Google BigQuery, Databricks, Cloud Storage to name a few. You can also write custom destinations if you wish. For most of my use cases revolved around populating a data lake in cloud storage. As such, I have made use of Azure Blob Storage as my destination for storing the data extracted from API responses. In the definition of a destination, you can specify what write disposition strategy to use, table name, path layout if it is a file system, and file format. DLT also stores the information about the pipeline state at this destination along with your data, which is used for performing incremental load.</p>
<h1 id="transformers">Transformers</h1>
<p>Have you come across a use case where you needed to get an ID from a endpoint, for instance ID of a customer, and use that to lookup details about the customer in a different endpoint. This is precisely the usage pattern where you would make use of <a href="https://dlthub.com/docs/general-usage/resource#process-resources-with-dlttransformer">transformers</a>, allowing you to feed data from one resource into another, which is quite powerful when dealing with standard RESTful APIs.</p>
<h1 id="optimization">Optimization</h1>
<p>Building a functional pipeline is a goal for many data engineers. However, functionality alone is not sufficient to make them reliable in production. First and foremost, the pipelines need be compatible with the available memory. Secondly, as previously discussed, it is important to optimize the use of compute resources, meaning we must parallelize as much as possible without compromising the overall performance. In DLT, you can optimize the performance of the pipeline, both its memory usage and parallelism, simply by turning the knobs on a handful of <a href="https://dlthub.com/docs/reference/performance">configurations</a> such as enabling file rotations, enabling/disabling file compressions, controlling the number of workers and more. Unfortunately, there are no magic formula for setting these parameters, and it is up to the engineers to iterate through them to figure out the most optimal configuration based on the business needs and resource availability. My recommendation here will be to familiarize yourself with these options while remembering to take into consideration about the different compute configuration that will be available to you in the local and production environment.</p>
<h1 id="closing-thoughts">Closing Thoughts</h1>
<p>My perspective on DLT remains optimistic. I foresee a future where DLT will be utilized by many organizations that are seeking control on building reliable and cost-efficient data pipelines. While there will always be cases where DLT’s current features might not fit specific needs, its Python-first design ensures extensibility. I’m excited to see how DLT evolves and how its adoption can empower more flexible data engineering solutions.</p>

    </div>
</div>

            </main>
        </div>


        <footer class="footer">
    
        <span>© 2024 Bijil Subhash</span>
    
</footer>
    </div>

</body>

</html>